!pip install sentence-transformers spacy requests
!python -m spacy download en_core_web_sm
import json
import os
import spacy
import requests
from sentence_transformers import SentenceTransformer, util

# ========== STORAGE SYSTEM ==========
class ResearchMemory:
    def __init__(self, filename="research_memory.json"):
        self.filename = filename
        self.data = self._load_memory()

    def _load_memory(self):
        if os.path.exists(self.filename):
            with open(self.filename, "r") as f:
                return json.load(f)
        return {}

    def save_result(self, query, result):
        self.data[query] = result
        with open(self.filename, "w") as f:
            json.dump(self.data, f, indent=4)

    def retrieve_result(self, query):
        return self.data.get(query, None)

memory = ResearchMemory()

# ========== SERPER SEARCH AGENT ==========
class WebSearchAgent:
    def __init__(self):
        self.model = SentenceTransformer("all-MiniLM-L6-v2")
        self.api_key = "..."  # <-- 🔐 Replace this with your real API key
        self.cache = {}
        self.headers = {
            "X-API-KEY": self.api_key,
            "Content-Type": "application/json"
        }

    def get_best_result(self, query):
        if query in self.cache:
            return self.cache[query]

        url = "https://google.serper.dev/search"
        response = requests.post(url, headers=self.headers, json={"q": query})

        if response.status_code != 200:
            return f"Search failed with status code {response.status_code}"

        data = response.json()
        results = [r["snippet"] for r in data.get("organic", []) if "snippet" in r]

        if not results:
            return "No relevant results found."

        query_embedding = self.model.encode(query, convert_to_tensor=True)
        result_embeddings = self.model.encode(results, convert_to_tensor=True)
        similarities = util.pytorch_cos_sim(query_embedding, result_embeddings)[0]
        best_result = results[similarities.argmax().item()]
        self.cache[query] = best_result
        return best_result

# ========== AGENTS ==========
class GenerationAgent:
    def __init__(self):
        self.web_search = WebSearchAgent()

    def process(self, query_data):
        query = query_data["query"]
        web_context = self.web_search.get_best_result(query)
        hypothesis = f"Based on current research, {query} may involve the following key findings: {web_context}"
        return {"hypothesis": hypothesis, "raw_data": web_context}

class ReflectionAgent:
    def __init__(self):
        self.nlp = spacy.load("en_core_web_sm")

    def process(self, hypothesis_data):
        doc = self.nlp(hypothesis_data["hypothesis"])
        has_subject = any(token.dep_ in ["nsubj", "nsubjpass"] for token in doc)
        has_verb = any(token.pos_ in ["VERB", "AUX"] for token in doc)
        coherent = has_subject and has_verb
        return {**hypothesis_data, "coherent": coherent}

class RankingAgent:
    def __init__(self):
        self.web_search = WebSearchAgent()

    def process(self, hypothesis_data):
        hypothesis = hypothesis_data["hypothesis"]
        support_evidence = self.web_search.get_best_result(hypothesis)
        confidence_score = len(support_evidence.split()) // 10
        score = min(confidence_score, 10)
        print(f"🔢 Ranking Score: {score}/10 for '{hypothesis}'")
        return {**hypothesis_data, "score": score}

class EvolutionAgent:
    def __init__(self):
        self.web_search = WebSearchAgent()

    def process(self, hypothesis_data):
        if hypothesis_data["score"] >= 6:
            return hypothesis_data
        best_search_result = self.web_search.get_best_result(hypothesis_data["hypothesis"])
        refined_hypothesis = f"{hypothesis_data['hypothesis']} Further verification shows: {best_search_result}."
        return {**hypothesis_data, "refined_hypothesis": refined_hypothesis}

class ProximityAgent:
    def __init__(self, memory):
        self.memory = memory
        self.model = SentenceTransformer('all-MiniLM-L6-v2')

    def process(self, hypothesis_data):
        stored_queries = list(self.memory.data.keys())
        if not stored_queries:
            return hypothesis_data

        query_embedding = self.model.encode(hypothesis_data["hypothesis"], convert_to_tensor=True)
        stored_embeddings = [self.model.encode(q, convert_to_tensor=True) for q in stored_queries]
        similarities = [util.pytorch_cos_sim(query_embedding, emb)[0][0].item() for emb in stored_embeddings]

        best_match_idx = similarities.index(max(similarities))
        if similarities[best_match_idx] > 0.7:
            past_research = self.memory.retrieve_result(stored_queries[best_match_idx])
            return {**hypothesis_data, "past_research": past_research}
        return hypothesis_data

class MetaReviewAgent:
    def process(self, hypothesis_data):
        if hypothesis_data["score"] >= 6:
            print("\n✅ Meta-Review Agent: Research is complete!")
            return {"Final Research Summary": hypothesis_data["hypothesis"], "status": "Completed"}

        print("\n🔄 Meta-Review Agent: Research needs improvement. Refining further...")
        return {
            "Final Research Summary": hypothesis_data["refined_hypothesis"],
            "status": "Needs Refinement"
        }

# ========== SUPERVISOR ==========
class Supervisor:
    def __init__(self):
        self.memory = ResearchMemory()
        self.agents = [
            GenerationAgent(),
            ReflectionAgent(),
            RankingAgent(),
            EvolutionAgent(),
            ProximityAgent(self.memory),
            MetaReviewAgent()
        ]

    def process_query(self, query):
        hypothesis_data = {"query": query, "original_query": query}
        max_iterations = 3
        iteration = 0

        while iteration < max_iterations:
            print(f"\n🔄 Iteration {iteration + 1}...")
            for agent in self.agents:
                hypothesis_data = agent.process(hypothesis_data)
                hypothesis_data["query"] = hypothesis_data.get("original_query", query)

            if hypothesis_data["status"] == "Completed":
                memory.save_result(query, hypothesis_data["Final Research Summary"])
                break
            iteration += 1

        print("\n Final Research Output:", hypothesis_data)

# ========== RUN ==========
supervisor = Supervisor()
user_query = input("Enter your research query: ")
supervisor.process_query(user_query)
